# -*- coding: utf-8 -*-
"""Pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C9yJuXSlcetvdZJtQU5vGSeIu8Tq5BTh
"""

!pip install transformers

import os

os.environ["HF_TOKEN"] = "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"

model_name = "google/gemma-3-1b-it"

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_name)

print(tokenizer("Hello, how are you?"))
print(tokenizer.get_vocab())

input_tokens = tokenizer("Hello, how are you?")["input_ids"]
print(input_tokens)

from transformers import AutoModelForCausalLM

import torch

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16
)

from transformers import pipeline

gen_pipeline = pipeline("text-generation", model=model, tokenizer=tokenizer)

gen_pipeline("Hey there", max_new_tokens=25)