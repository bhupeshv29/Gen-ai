# -*- coding: utf-8 -*-
"""Pipeline v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15rfchi-okbLLNIqmi1RUMqQ8GorWxtgu
"""

!pip install transformers

import os

os.environ["HF_TOKEN"] = "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"

from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "google/gemma-3-1b-it"

tokenizer = AutoTokenizer.from_pretrained(model_name)

input_prompt = [
    "The capital of India is"
]

tokenized = tokenizer(input_prompt, return_tensors="pt")

tokenized["input_ids"]

import torch

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16
)

gen_result = model.generate(tokenized["input_ids"], max_new_tokens=25)

gen_result

ouput = tokenizer.batch_decode(gen_result)
ouput